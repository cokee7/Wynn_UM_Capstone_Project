Original URL: https://wallstreetcn.com/articles/3745522
--------------------------------------------------
Content:
姚顺雨表示，AI接下来比拼的不是训练，而是“如何定义并评估真正有用的任务”，得像产品经理一样重新思考：AI究竟该为谁解决什么问题、又该如何衡量“解决得好不好”。
还记得 AlphaGo 刚击败李世石时，全世界惊呼“人工智能时代来了”，转眼不过几年，ChatGPT、o‑系列 模型已经把“智能”从棋盘和试卷一路卷到代码、创作甚至电脑屏幕背后的每一次点击
清华姚班出身大牛，现任OpenAI 研究科学家
姚顺雨
在最新长文《The Second Half》中抛出一个惊人判断：
过去几十年我们专注于“把模型训得更强”，如今游戏规则彻底反转——接下来比拼的不是训练，而是“如何定义并评估真正有用的任务”。换言之，第一阶段的看家法宝是 Transformer、深度强化学习和大规模预训练；而第二阶段，你得像产品经理一样重新思考：AI 究竟该为谁解决什么问题、又该如何衡量“解决得好不好”。这一转向，将决定谁只是“模型分数更高”，谁能真正撬动万亿级经济价值
姚顺雨在文中还提到：
Sutton（强化学习之父） & Barto 的经典教材几乎只谈算法，几乎不谈环境与先验，然而，在
深度 RL
时代，人们发现环境对经验结果影响巨大：一种算法的表现往往极度依赖其开发和测试的环境。如果忽略环境，你也许会造出只在玩具设置里无比优越的“最优”算法。那么为何不先确定真正想解决的环境，再找最适合的算法？
这段看法其实刚好和这两天
Sutton与谷歌RL副总裁写的最新论文《
Welcome to the Era of Experience》的观点一致
以下是全文分享：
《The Second Half》全文翻译
原作者：
姚顺雨（Shunyu Yao），OpenAI 研究科学家
原文标题：
The Second Half
原文摘要：
We’re at AI’s halftime.
地址：
https://ysymyth.github.io/The-Second-Half/
几十年来，AI 主要关注于开发新的训练方法和模型。事实证明这条路行之有效：从击败世界冠军的国际象棋和围棋程序，到在 SAT 和律师资格考试上超过大多数人类，再到在 IMO 和 IOI 上摘金夺银。写进教科书的里程碑——Deep Blue、AlphaGo、GPT‑4 以及 o‑series——背后都是 AI 方法上的根本性创新：搜索、深度强化学习、规模化和推理能力。随着时间推移，一切都在变得更好。
那么，现在究竟发生了什么不同？
用三个词概括：
强化学习终于奏效了
。更准确地说：
强化学习终于具备了泛化能力
。经历多次重大弯路和里程碑的累积，我们终于找到了一套可行的配方，能用语言和推理解决各种 RL 任务。哪怕就在一年前，如果你告诉大多数 AI 研究者一份单一的配方可以同时搞定软件工程、创意写作、IMO 级数学、键盘鼠标操作以及长篇问答——他们肯定会笑你“幻觉”。这些任务各自极难，许多研究者整个博士阶段可能只盯着其中一个小方向。
然而，它真的发生了。
接下来会怎样？
AI 的
下半场
——从此刻开始——将把重心从“解决问题”转向“定义问题”。在这个新时代，
评价
比
训练
更重要。我们不再仅仅问“能不能训练出解决 X 的模型？”，而是要问“我们应该训练 AI 做什么？如何衡量真实的进步？”要在下半场取得成功，我们必须及时转变心态和技能，更像产品经理那样思考。
前半场回顾
要理解前半场，就看看哪些工作赢得了桂冠。你认为迄今最具影响力的 AI 论文是哪几篇？
我在斯坦福 224N 课堂上做过小测验，答案并不意外：Transformer、AlexNet、GPT‑3 等。这些论文的共同点是什么？
它们提出了能训练出更好模型的根本性突破
，并通过在某些基准上显著提升成绩来发表。
还有一个潜在共性：这些“赢家”都是
训练方法或模型
，而不是
基准或任务
。哪怕可以说是最具影响力的基准数据集——ImageNet——其引用量也不到 AlexNet 的三分之一。模型 VS. 基准的对比在其他地方更为悬殊：Transformer 的核心基准是 WMT’14 翻译，其研讨会报告引用量约 1300，而 Transformer 论文则超过 160,000。
这说明了前半场的游戏规则：
重点是构建新模型和方法
，而评估与基准处于次要（但必要）地位。
为什么？因为在 AI 的前半场，提出新算法或模型架构往往比定义任务更难、更令人兴奋。与此相对，把已有的人类任务（翻译、图像识别、下棋）转成基准显得简单得多。更重要的是，
好方法往往更通用
：Transformer 最初在 WMT’14 翻译任务上崭露头角，后来却驱动了计算机视觉、自然语言处理、强化学习等众多领域的进步。一个伟大的新方法能爬过许多不同的基准，因此其影响通常超越单一任务。
这套游戏行之数十年，催生了改变世界的想法与突破，在各领域不断刷新基准成绩。为何游戏要改变？因为所有这些突破的累积，带来了能够
真正解决任务的“通用配方”
。
配方是什么？
配方的原料并不意外：
大规模语言预训练、数据与计算的规模化，以及“推理与行动”的理念
。这些词似乎成了旧金山每天都在喊的流行语，但为何称之为“配方”？
可以用强化学习 (RL) 的视角来理解——RL 常被视作 AI 的“终局”——理论上能赢下游戏，实际上也难以想象没有 RL 的超人系统（如 AlphaGo）。RL 有三大关键组成：
算法、环境与先验
。长期以来，RL 研究者几乎把全部注意力放在算法（REINFORCE、DQN、PPO、TRPO 等）上，同时把环境和先验视为固定或最小化条件。Sutton & Barto 的经典教材几乎只谈算法，几乎不谈环境与先验。
然而，在
深度 RL
时代，人们发现环境对经验结果影响巨大：一种算法的表现往往极度依赖其开发和测试的环境。如果忽略环境，你也许会造出只在玩具设置里无比优越的“最优”算法。那么为何不先确定真正想解决的环境，再找最适合的算法？
这正是 OpenAI 的初衷：他们构建了 Gym、World of Bits、Universe 等一系列标准 RL 环境，试图把互联网或电脑变成游戏环境。计划听上去完美：一旦把所有数字世界变成环境，再用聪明的 RL 算法解决它们，就能得到数字 AGI。
计划很好，但并不完全奏效。OpenAI 在用 RL 解决 Dota、机械手等方向取得巨大进展，却始终无法搞定“用电脑”或“网页导航”，而且一个领域的 RL 代理无法迁移到另一领域。缺了什么？
直到 GPT‑2、GPT‑3 出现，人们才发现缺的原来是
先验
。需要强大的语言预训练，把常识和语言知识蒸馏进模型，再通过微调把它变成 WebGPT 或 ChatGPT（并改变世界）。事实证明，RL 最重要的部分可能并非算法或环境，而是先验——而这些先验可以通过与 RL 并不直接相关的方式获得。
语言预训练为聊天提供了好先验，却不足以同样出色地操控电脑或玩电子游戏。为何？因为这些领域与互联网文本分布差得更远，直接进行监督微调或 RL 效果不佳。2019 年 GPT‑2 刚问世时，我曾在此之上做监督微调／RL 来解决文本冒险游戏——CALM 是世界上第一个基于预训练语言模型的游戏代理。但它需要数百万步 RL 才能爬过单个游戏，且无法泛化。虽然这正是典型 RL 的特征，但我觉得奇怪：人类却能零样本上手新游戏并表现更好。于是我迎来了人生第一次“顿悟”：
我们之所以泛化，是因为我们可以选择“思考”而不只是“行动”
——例如先想到“地牢危险，需要武器，而箱子可能藏武器”，再规划行动。
“思考”，或“推理”，是一种奇特动作：它不直接影响外部世界，但其空间开放、组合爆炸——你可以想一个词、一句话、一段话，甚至随机想 10000 个单词，而周围世界不会立即改变。在经典 RL 理论中，这是交易极差、让决策几乎不可能：如果需要在两个箱子中选一个，有一个有 100 万美元，另一个空，你期望赚 50 万；若我再加无限个空箱子，你期望收获为零。但当把推理加入 RL 中的动作空间时，我们借助语言预训练先验来泛化，并能为不同决策灵活配置推理时长。这很神奇，恐怕我得另写文章解释；简而言之：
语言通过代理中的推理实现泛化
。
当我们拥有正确的 RL 先验（语言预训练）和 RL 环境（把语言推理作为动作）后，RL 算法反而成了最琐碎的部分。于是有了 o‑series、R1、“deep research” 和面向电脑操作的代理，未来还会更多。讽刺的是：几十年来 RL 研究者过分关注算法，几乎没人理会先验——所有实验几乎都是从零开始。却花了数十年弯路才发现，也许我们该完全倒过来排优先级。
正如 Steve Jobs 所说：
“你无法预见地连接点点滴滴，只有回头看时才能。”
下半场
这套配方正在彻底改写游戏规则。回顾前半场的循环：
1. 提出新训练方法或模型，刷基准分数；
2. 创建更难的基准，继续循环。
现在循环被破坏了，因为：
配方把“刷分”工业化且无需太多新点子。你琢磨半天提升 5%，下一代 o‑series 随手提升 30%。
即便造更难基准，配方很快（而且越来越快）就能解决。
接下来怎么玩？如果新方法不再稀缺，而更难基准也会迅速被破，那我们该做什么？
我认为必须从根本上重新思考“评估”
。这不仅仅是再造新基准，更要质疑现有评估设置，创造新的评估方式，迫使我们发明超越配方的新方法。这很难，因为人类有惯性，很少质疑基本假设——许多假设被视为天经地义。
举两例说明惯性：
1.
评估“应该”自动运行
——通常代理收到一次任务输入，就完全自主地做事，最后得到评分。但现实中，代理必须在任务过程中与人类交互——你不会给客服发一大段信息等 10 分钟就指望一次性解决。于是出现了让真人或仿真用户在环的基准：如 Chatbot Arena、tau‑bench。
2.
评估“应该”独立同分布 (i.i.d.) 运行
——如果测试集有 500 个任务，你并行跑完取平均。但现实中任务是顺序完成的：Google 软件工程师越熟悉代码库，解决 bug 越快；而代理却在同一仓库里反复“首次见面”。我们显然需要长期记忆，但学术界缺乏能证明需求的基准，也缺乏质疑 i.i.d. 假设的勇气。
这些假设在前半场无伤大雅，因为智能水平低时，只要智能提升，效用就会同步提升。但现在，通用配方已确保在这些假设下必胜。
因此，下半场的新游戏是：
1.
设计面向真实效用的新评估设置或任务
；
2.
用配方或在其上增添新组件来解决它们
，循环往复。
这场游戏难在陌生，却也令人兴奋。前半场玩家在电子游戏和考试里刷分；下半场玩家则有机会把智能做成真正有用的产品，建立十亿、万亿美元的公司。前半场充斥增量模型和方法；在下半场，它们被配方“过滤”——除非你创造新假设来打破配方，否则注定被碾压。
欢迎来到下半场！
鸣谢
本文基于作者在 Stanford 224N 与 Columbia 的演讲。初稿由 OpenAI “deep research” 读取幻灯片并生成。
本文来源：
AI寒武纪
，原文标题：《清华姚班大牛，OpenAI姚顺雨：AI的下半场要像产品经理一样思考，RL算法取决于环境》
风险提示及免责条款
市场有风险，投资需谨慎。本文不构成个人投资建议，也未考虑到个别用户特殊的投资目标、财务状况或需要。用户应考虑本文中的任何意见、观点或结论是否符合其特定状况。据此投资，责任自负。